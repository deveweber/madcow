from dotenv import load_dotenv
import os
import uuid

import chainlit as cl

from langchain_core.messages import HumanMessage, AIMessage

from workflow import create_graph, CollaborativeState, AGENTS

load_dotenv()

import logging

# log_level = os.getenv('LOG_LEVEL', 'INFO')
# logging.basicConfig(level=log_level)
# logger = logging.getLogger(__name__)

# Ensure you have set your OpenAI API key in your environment variables
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# Chainlit UI setup
@cl.on_chat_start
async def start():
    _, graph = create_graph()

    config={"configurable": {"thread_id": str(uuid.uuid4())}}

    await cl.Message(content="This panel discussion is entirely fictional. The responses are generated by AI and do not represent the actual views or opinions of the individuals mentioned.").send()

    state = CollaborativeState(messages=[], lead_agent=[list(AGENTS.keys())[0]], agent_contributions=[], human_inputs=[])
    graph.invoke(state, config=config)

    cl.user_session.set("graph", graph)
    cl.user_session.set("config", config)

@cl.on_message
async def on_message(message: cl.Message):
    config = cl.user_session.get("config")
    graph = cl.user_session.get("graph")
    graph.update_state(config, {"human_inputs": [message.content]})

    ui_messages = {}
    # Run the graph
    async for event in graph.astream_events(None, config=config, version="v2"):
        if event["event"] == "on_custom_event":
            agent_name = event["data"]["agent_name"]
            if event["name"] == "contributor_agent_executor":
                ui_message = cl.Message(author=agent_name, content=f"@{agent_name}: ", type="assistant_message")
            else:
                ui_message = cl.Message(author=agent_name, content=f"@{agent_name}: ", type="assistant_message")
            ui_messages[event["metadata"]["langgraph_checkpoint_ns"]] = ui_message
        if event["event"] == "on_chat_model_stream":
            ui_message = ui_messages[event["metadata"]["langgraph_checkpoint_ns"]]
            await ui_message.stream_token(token=event["data"]["chunk"].content)
        if event["event"] == "on_chat_model_end":
            ui_message = ui_messages[event["metadata"]["langgraph_checkpoint_ns"]]
            await ui_message.send()

if __name__ == "__main__":
    cl.run()
